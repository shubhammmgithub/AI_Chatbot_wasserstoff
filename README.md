# ğŸ“„ AI Document Research & Theme Identification Chatbot

This project is an advanced, session-based RAG (Retrieval-Augmented Generation) application that allows users to have a stateful conversation about their documents. It features a private, temporary knowledge base for each user, ensuring data privacy. The system can ingest multiple document formats (including scanned PDFs with OCR), provide LLM-synthesized answers with accurate citations, and perform a deep analysis to automatically name and summarize common themes.

This project was completed as part of the AI Engineer Intern Task for Wasserstoff.

----------------------------------------------------------------------------------------------------------------------

## âœ¨ Features


* **Conversational Memory**: The chatbot remembers the last 20 messages in a session, allowing for natural follow-up questions.

* **Private User Sessions**: Each user gets a temporary, private knowledge base that is cleared when they end their session.
* **Multi-Format Document Ingestion**: Supports PDFs, DOCX, TXT, and scanned images (PNG, JPG) with built-in OCR.
* **Synthesized, Cited Answers**: Uses a powerful LLM to generate conversational answers based only on the provided documents, complete with citations.
* **Automated Theme Analysis**: Can perform a deep analysis to find, name, and summarize common themes across all documents in a session, with results streamed in real-time.
* **Modern Tech Stack**: Built with a robust FastAPI backend and an interactive Streamlit frontend.

----------------------------------------------------------------------------------------------------------------------


## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
| :--- | :--- | :--- |
| **Frontend** | Streamlit | To create a fast, interactive web-based user interface. |
| **Backend API** | FastAPI | For a high-performance, scalable, and modern API. |
| **Vector DB** | Qdrant | To store document embeddings and perform efficient similarity searches. |
| **LLM** | Groq (Llama 3) | For fast and high-quality answer synthesis and theme analysis. |
| **AI Framework**| LangChain | To orchestrate the interactions with the LLM. |
| **Embedding Model**| all-MiniLM-L6-v2 | To convert text chunks into high-quality numerical vectors. |
| **OCR Engine** | Tesseract | To extract text from scanned documents and images. |


======================================================================================================================


## ğŸš€ Getting Started

These instructions will get you a copy of the project up and running on your local machine.

### Prerequisites

* Python 3.9+
* An API key from [GroqCloud](https://console.groq.com/keys).

### Installation & Setup

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/shubhammmgithub/AI_Chatbot_wasserstoff
    ```

2.  **Create a virtual environment** inside the `backend` directory:
    ```bash
    python -m venv backend/.venv
    ```

3.  **Activate the virtual environment:**
    ```bash
    # On Windows
    .\backend\.venv\Scripts\activate
    ```

4.  **Install the required packages:**
    ```bash
    pip install -r requirements.txt
    ```

5.  **Set up your environment variables:**
    * Create a file named `.env` in the root project directory.
    * Add your Groq API key to this file:
        ```env
        GROK_API_KEY=gsk_YourActualApiKeyGoesHere
        ```

======================================================================================================================



### Running the Application

You must have **two terminals** open to run the application.

**Terminal 0 : If qdrant is running from docker then**
```
docker run -p 6333:6333 qdrant/qdrant
```


**Terminal 1: Start the Backend Server**
```bash
# (From the project root, with .venv activated)
this is how terminal will look like when you will enter the command
(.venv) PS D:\wasserstoff_ai> run this command - uvicorn backend.app.api.app:app --reload --port 8000

command:uvicorn backend.app.api.app:app --reload --port 8000
```
The backend will be running at `http://127.0.0.1:8000`.

**Terminal 2: Start the Frontend Application**
```bash
# (From the project root, with .venv activated)
this is how terminal will look like when YOU WILL type the command 
(.venv) PS D:\wasserstoff_ai> streamlit run frontend/streamlit_app.py

command:
streamlit run frontend/streamlit_app.py
```
A new tab will open in your browser with the user interface.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


ğŸš€ Application Workflow
This application follows a modern Retrieval-Augmented Generation (RAG) pipeline to provide answers based on user-provided documents. All data is handled in private, temporary user sessions.

Query Flow
Ingestion: The user uploads multiple documents. The system extracts text (using OCR if necessary), splits it into chunks, and converts each chunk into a vector embedding using a SentenceTransformer model.

Indexing: These embeddings and their corresponding text are stored in a private, session-specific collection in a Qdrant vector database.

Search: When the user asks a question, the query is also converted into a vector embedding. The system then performs a similarity search in the user's private Qdrant collection to find the most relevant document chunks.

Reranking: The initial search results are re-ordered using a more precise similarity calculation to improve accuracy.

Synthesis: The top-ranked, most relevant chunks are compiled into a context and sent to a Large Language Model (Groq/Llama 3). The LLM generates a final, conversational answer based only on this context and includes citations.

Display: The synthesized answer and the supporting document chunks are displayed to the user.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

ğŸ“ Final Project Structure
The project is organized into a frontend and a modular backend, ensuring a clean separation of concerns.

wasserstoff-ai-chatbot/
â”œâ”€â”€ .gitignore               # Specifies files for Git to ignore (e.g., .env, .venv).
â”œâ”€â”€ README.md                # Project documentation.
â”œâ”€â”€ requirements.txt         # Lists all Python package dependencies.
â”œâ”€â”€ .env                     # Stores secret keys (e.g., GROK_API_KEY).
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ streamlit_app.py     # Contains all the code for the Streamlit user interface.
â”‚
â””â”€â”€ backend/
    â”œâ”€â”€ .venv/               # The project's isolated Python virtual environment.
    â”œâ”€â”€ main.py              # A simple, configurable script to launch the backend server.
    â”‚
    â””â”€â”€ app/
        â”œâ”€â”€ api/
        â”‚   â””â”€â”€ app.py       # FastAPI application initialization, CORS, and middleware.
        â”œâ”€â”€ core/
        â”‚   â””â”€â”€ config.py    # Central configuration for models, DBs, and other settings.
        â”œâ”€â”€ models/
        â”‚   â””â”€â”€ schemas.py   # Pydantic models for API request and response validation.
        â”œâ”€â”€ routes/          # Defines all the API endpoints.
        â”‚   â”œâ”€â”€ _init__.py  # Assembles all the individual routers into one.
        â”‚   â”œâ”€â”€ admin.py     # Endpoint for clearing a user's session data.
        â”‚   â”œâ”€â”€ ask.py       # Endpoint for handling user questions.
        â”‚   â”œâ”€â”€ ingest.py    # Endpoint for document ingestion.
        â”‚   â””â”€â”€ themes.py    # Endpoints for theme counting and analysis.
        â””â”€â”€ services/        # Contains the core business logic.
            â”œâ”€â”€ embedding_service.py   # Handles text embedding, clustering, and saving to Qdrant.
            â”œâ”€â”€ extraction_service.py  # Handles text extraction and chunking from files.
            â”œâ”€â”€ retrieval_service.py   # Implements the core RAG pipeline (search, rerank, synthesize).
            â””â”€â”€ theme_service.py       # Handles the logic for naming and summarizing themes.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
